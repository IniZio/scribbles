---
title: Comp2611
desc: L
category: Study
tags: [ comp2611 ]
date: 2016-11-17
filename: 2016-11-17-Comp2611
---

https://course.cse.ust.hk/comp2611/note/COMP2611_Memory_Fall2016_part1.pdf

### Cache

#### Block placement

###### Mapping

==Direct-mapped (1 way)== : one memory match to one possible cache block

​	Method 1 (most common): ==cache_location = block_address MOD number_of_blocks_in_cache==

​		number of cache blocks (N) is $2^m$

​	Method 2: cache_location = if m cache sets, lower order m bits of the block address

​		can avoid latency of MOD
​		compare the tag i.e. the lower m bits, with the memory addresses to see if the block address is actually in memory

​	Example: `lw $t0, 1200($zero)` with direct-mapped cache

​		block address: 	block containing all addresses 

​						between [byte address/ bytes per block] $\times$ byte per block    & 

​								 [byte address/ bytes per block] $\times$ byte per block + [byte per block + 1]

​		memory address generated by CPU = 1200.

​		block address for byte address 1200 = floor(1200/32)

​		it is sent to cache to look for data

​		by direct mapping, the data block is in entry = floor(1200/32) MOD 8 = 5

​	(P.30) What is cache frames?

​	Disadvantage of DM (P. 34):  for new mapped block to same cache frame, have to kick out existing one instead of using other 			empty cache blocks

Cache size: total size of the cache

Have optimal block size to get min. miss rate with relatively good compromise. Too little then not taking full advantage of locality. Too great then too frequent swap in and out

==Full associative (All ways)==: each cache block can be placed ==anywhere== in cache instead of matching one to one

​	Better than Direct-mapping:

​		No cache conflict $\to$ $\uparrow$ cache hit rate. Still have misses due to size aka. capacity miss

​	Worse than Direct-mapping:

​		Need better hardware and time to find a block in cache

==Set associative (N ways)==: each cache block can be placed certain number of locations in cache. Compromise between DM and FA

Example: 2-way set-associative

​	Associativity = 2

\# of cache sets = cache size / cache block size / ==N==

#### Block identification

To tell which block is in location i.e. is it hit or miss?

Direct mapping: | index || valid || tag || data|

​	==Tag== to store address information. It contain high-order bits not used as index. It is residue after chopping off byte offset and index

​	==valid== to indicate whether cache block has valid data

Example: DM cache with 16 KB f data and 8-word blocks

block size of 8 words  $\to$ $2^9$ blocks

==Tag== has 32 - 9 (the 9 above)  - power of (8 word) for 2 i.e. 5 = 18 bits

==Parallel lookup== means looking at all possible locations in memory and only one will be valid i.e. right one. 

How CPU handle cache miss (P.47): make a request to refill cache and CPU is stalled until 1st level cache is filled. Does not need saving state of registers

> instructions can also be cached

#### Block replacement

Candidates for differrent mapping. 1 for DM, all for FA, N for SA

Strategies:

​	==Random==

​	==Least recently used==: the most AFK block. Costly for high associativity