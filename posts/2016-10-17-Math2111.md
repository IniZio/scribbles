---
title: Math2111
category: Study
tags: [ math2111 ]
date: 2016-10-17
filename: 2016-10-17-Math2111
---

## L2

- Coeffienient Matrix: put the left side of system of equations in matrix
  $\begin{pmatrix} 1 & 1 & 0 \\2 & 2 & 9\end{pmatrix} $
- Augumented Matrix: put the whole system of equations in matrix, can solve system of equations
  $\begin{pmatrix} 1 & 1 & 0 | 10\\2 & 2 & 9 | 23\end{pmatrix} $
- Any matrix is a row echelon form (REF) (i.e. the system of equations is  solved) if satisfy one of:
  - zeros only appear at the buttom
  - each non-zero entry most to right at least one number ahead of upper row
  - All entries in a column below a leading entry must be zeros
- Any matrix is Reduced REF if satisfy REF and one of:
  - leading non-zero entry must be 1
  - 1 is the only non-zero entity

## week202 L3

- pivot position: the postions of leading 1's in the RREF of Matrix
- Systematic way of solving equations:
  - write the system in aug. matrix (M$|$b)
    - (M$|$b) EROs $\rightarrow$ (M'$|$b')
  - spot the pivot pos. and pivot cols.
  - if b' contains a pivot pos, it is not consistent. Otherwise Consistent
    - if consistent, then look at M': if it has free col, then it has infinite solutions, else has unique

## week202 T1

- email: smmcdonnell@ust.hk

- $\begin{pmatrix}1 & 3 & -2 & 5 | 4\\2 & 8 & -1 & 9 | 9\\3 & 5 & -12 & 17 | 7\end{pmatrix}$

  $\to$
   $\begin{pmatrix}1 & 3 & -2 & 5 | 4\\0 & 2 & 3 & -1 | 1\\0 & -4 & -6 & 2 | -5\\\end{pmatrix}$

   $\longrightarrow$
    $ \begin{pmatrix}1 & 0 & -13/2 & 13/2 | 5/2\\0 & 1 & 3/2 & -1/2 | 1/2\\0 & 0 & 0 & 0 | -3\\\end{pmatrix}$


- Linear combination: Let S be a collection of vectors, a linear combination of S is a vector that can be reached by scaled vectors in S  e.g.
   $\begin{pmatrix}2 \\4\\\end{pmatrix}$

   is a linear combination of 

  {$\begin{pmatrix}1 \\2\\ \end{pmatrix} $,$\begin{pmatrix}0 \\1\\0 \\ \end{pmatrix}$}

   is a linear combination of 

  {$ \begin{pmatrix}1 \\0\\0\\\end{pmatrix}$, $\begin{pmatrix}1 \\1\\0\\\end{pmatrix}$}

- Linear span of S (Span(S)) is all possible linear combinations of S

- Finding linear combination of vectors = solving system of equations

## L5

- Homogenous equation: a matrix equation/system of linear equations of form Ax = 0. It is always consistent since all x_i s = 0 will solve
- In order to have non-zero (non-trivial) solutions for Homogenous equations, we need to see if there are free columns in A

## T2

- Rank: number of pivots in a matrix
- Mid-term Q: find a condition on $b_i$ such that the system is consistent
- To find number Free variables: RREF the matrix, find the pivot columns, find the free columns, count the free variables
- for an augumented matrix, if each column contains a pivot, then it has no solution (since there will be a row like $0x_1+0x^2+0x^3=1$)

## L6

### Non-homogenous equation

- Ax = b (b != 0)
- Not always consistent
- solutions not of the form span{$x_1$, $x_2$...$x_k$}

#### One way to solve non-homogenous equations:

1. Find a particular solution of it
2. solve the associated homogenous equation Ax=0

##### E.g. $x_1+x_2+x_3+x_4=5$ and $x_3+2x_4=3$

###### Solution:

1. $\begin{pmatrix}x_1\\x_2\\x_3\\x_4\\\end{pmatrix}$
   =
   $\begin{pmatrix}1\\2\\3\\4\\\end{pmatrix}$
   is a solution of the eq.
2. solve $\begin{pmatrix}1 & 1 & 1 & 1 | 0 \\0 & 0 & 1 & 2 | 0\end{pmatrix}$
   $\rightarrow$ span{$\begin{pmatrix}1 \\ 1\\0\\0\end{pmatrix},\begin{pmatrix}1 \\ 0\\-2\\1\end{pmatrix}$}
   $
3. all solutions are of the form $\begin{pmatrix}1 \\ 2\\1\\1\end{pmatrix}+\begin{pmatrix}-1 \\ 1\\0\\0\end{pmatrix}+\begin{pmatrix}1 \\ 0\\2\\1\end{pmatrix}$

##### Reason that solution works

1 says that x=$u_0$ is a solution. 2 says that $underline{v}$ is a solution of Ax=0. We claim that x=$u_0$+v is the solution
Proof: LHS = A($u_0$ + v) = A$u_0$ + Av = b + 0 = b = RHS

##### Alternate solution:

solve with matrix normally, use S and T to substitute $x_2$, $x_4$, will get the same thing

### Span

- span as a noun: e.g. A = $\begin{pmatrix}1&1&1\\2&2&2\\3&3&3\end{pmatrix}$, span of A is span{\begin{pmatrix}1\\2\\3\end{pmatrix}}}, which does not span $R^3$
- span as verb: A matrix spans $R^N$ if the matrix can go everywhere in the dimension

### Equavalent throrems of a matrix (one decides whether others are true)

- Ax=b is consistent for each b in $R^m$
- each b in $R^m$ is a linear combination of the columns of A
- The columns of A spans $R^m$
- A has a pivot position on every row

## L7

### Linear dependence

- given S = {$v_1$, $v_2$,....$v_k$}, it is linear dependent if there are redundant directions
- e.g. S = {$\begin{pmatrix} 1\\2\\3 \end{pmatrix}$, $\begin{pmatrix} 1\\2\\3 \end{pmatrix}$,$\begin{pmatrix} 1\\2\\3 \end{pmatrix}$}. It is linear dependent
- a vector is redundant in S if it can be linearly combined by other vectors in S

### Week4 note Exercises:

1. yes
2. yes
3. yes

#### If there is a height $>$ width matrix (e.g. 4*3 matrix), then it is linearly dependent if there are 3 pivot columns

### Matrix transformation

- For the transformation input: x ∈ $R^n$ $\rightarrow$ output Ax ∈ $R^m$, we call this function T: $R^m \rightarrow R^n$
- Function T is a linear transformation if:
  - T(x+y) = T(x) + T(y)
  - T(c*x) = c*T(x)

### Week4 note exercise(2)

1. T(x) = Ax = A$\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}$
2. i.e. which input x gives output 0. So just solve Ax=$\begin{pmatrix}0\\0 \end{pmatrix}$
3. i.e. same as 2 except R.H.S=$\begin{pmatrix}\end{pmatrix}$

## T3

1. i.$\begin{pmatrix} 1 & 0 &1& 1& 2\\ 0 & 2 & 1 & 2& 1 \end{pmatrix}$
   - write $x_1, x_2$ as "f'n" of others
   - $x_1 = -x_3 -x_4 -2x_5$
   - $x_2 = -.5x_3-x_4-.5x_5$
   - $\begin{pmatrix}x_1\\x_2\\x_3\\x_4\\x_5\\ \end{pmatrix} = x_3\begin{pmatrix}-1&-.5&1&0&0\end{pmatrix}$+...
   - basis: a linearly independent spanning set for a vector space ie from the basis you can create some vector space
   - basis for i) is \begin{pmatrix}-1 & -.5 & 1 & 0 & 0 \end{pmatrix},...(too slow :P)
2. i) repeat 1.
3. Check if LI/LD
   - $\{\begin{pmatrix}1\\1\end{pmatrix}\}$ we require c*this = 0 iff c = 0, then its LI. since its != $\begin{pmatrix}0\\0\end{pmatrix}$ matrix, c=0. so Yes
   - $\{\begin{pmatrix}1\\-1\\1\\2\end{pmatrix}, \begin{pmatrix}3\\-2\\1\\3\end{pmatrix}, \begin{pmatrix}5\\-2\\-1\\1\end{pmatrix}\}$: check if LI by row reducing against 0. Have free column, therefore not LI
4. u=x+y, v=x-y, w=2x+3y. Check all pair possibilities check if LI: (recall if LI then all $C_i$s=0)
   - {u,v}: $c_1(x+y)+c_2(x-y)=0$
     $x(c_1+c_2)-y(c_1+c_2)=0$
     if $c_1=c_2$,
     2$c_1$x=0, implies x=0 which contradicts
   - {u,v,w}: assuming its LI, $c_1u+c_2v+u_3w=0$
     plug in all the x's and y 's
     (c_1+c_2+2c_3)x+(c_1+c_2+3c_3)y=0
     should hold because {x,y} are LI
     make a system of coeffiencts with c's to see if =0, solve it
     $\rightarrow$ $\begin{pmatrix}c_1\\c_2\\c_3\end{pmatrix} \rightarrow \begin{pmatrix}-2.5\\.5\\1\end{pmatrix}c_3$.
     implies that {x,y} are LD, which contradicts. Therefore is LD
5. i) for a augumented matix fron LI set,
   - $c_1\begin{pmatrix}1\\0\\0\end{pmatrix} - 0$ iff c_1 = 0

## L8

### injectiveness and surjective

- One-to-one(injective): only one arrow come from each left x element. f(x) = f(y), x=y
- Onto(surjective): only one arrow arrive each right y element.

#### theorem of inverse

- IF $X \rightarrow Y$ is both injective and surjecive, then we have inverse $f^{-1}: Y \rightarrow X$

#### when is T(x)=Ax not trivial?? (T: $R^n \rightarrow R^m$)

- Suppose Ax=0 has a non-trivial solutions x=u != 0
- $\rightarrow$ Then there are more than two solutions where 0=Au,
- $\rightarrow$ T(x)=Ax is not injective

#### When is T(x)=Ax surjective?

- For only b $\exists R^m$, there is an $x_0 \exists R^n$ so that:
- A$x_0$ = b $\Rightarrow (A|b) is consistent for all b$ $\Rightarrow$ columns of A spans R^m

#### week4 P.8 has a table for linear transformations, vectors, matrixes

#### E.g. T: $R^2 \rightarrow R^3$, $T\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}x\\y\\0\end{pmatrix}$, Is it injective or surjective?

- Standard of T:
- A = $\begin{pmatrix}T\begin{pmatrix}1\\0\end{pmatrix}}$T$$\begin{pmatrix}0\\1\end{pmatrix}\end{pmatrix}$
- Check if T is injective $\Rightarrow$ Check if columns of A is linearly independent
- Since it is L.I., it is injective
- T is surjective $\Rightarrow$ Check if A has pivot on **all rows**.
- Since it doesnot have pivot on last column, it is not surjective

### Injective/Surjective in transforming dimension

- No two arrows eading the ssame target on RHS $\rightarrow$ injective
- LHS dimensoin cannot cover te whole RHS dimension $\rightarrow$ not surjective

## T4

### E.g.1 Identiry the domain/codomain of the matrix transformations given by following:

1. A = [1,2,1\\1,2,2]
2. [1,2,1,1\\1,2,0,1\\0,2,1,2\\1,-2,2,-2]
3. [1,0,2,-2\\3,1,4,-6\\0,2,-4,1\\-1,3,5,2\\0,3,7,1]

### E.g.2 Are the linear transformations in E.g.1 one-to-one or onto?

- Theorem: let A be a standard mtx of a LT T: $R^n\rightarrow R^m$
  1.T is onto $\equiv$ cols of A span $R^m$ $\equiv$ pivot in every row
  1. T is one-to-one $\equiv$ cols of A are LI $\equiv$ pivot in every col

1. check if cols span $R^2$. A has pivots on every colmn but has free variable. So it is onto but not one-to-one
2. onto, not one-to-one
3. one-to-one, not onto

### E.g.4 $T: R^n \rightarrow R^m$. what can you say about m,n?

i.

- T is one-to-one
- if has pivots in ever cols then n $<=$ m
  - i.e. less cols than rows $\equiv$ less variables than equations

j.

- T is onto
- pivots in every row
  - $\Rightarrow$ n $>=m$ $\equiv$ more variables than eqns

### E.g.3 $T: R^n \rightarrow R^m$: T[1\\2] = [3\\1\\-5], T[-1\\1]=[0\\-4\\2]. FInd the standard matrix A of T. Is T one-to-one or onto?

- definition matrix A s.t. Tx = Ax $\forall$ X $\exists R^n$ is the standard matrix of T, i.e. A = {$Te_1....,Te^n$}
- A(1,-1\\2,1) =  [3,0\\1,-4\\-5,2]
- A = [3,0\\1,-4\\-5,2]$[1,-1\\2,1]^-1$
- A = [1,1\\3,-1\\-3,-1]
- = [1,0\\0,1\\0,0]
- Therefore not onto but is one-to-one

### E.g.10 Let E  [0,0,0\\5,0,0\\0,0,0], A be any 3*4 matrix

1. compute EA, describe it

- $\Rightarrow$ $[0,0,0,0\\5a_{11},5a_{12},5a_{13},5a_{14}\\0,0,0,0]$. We have $5r_1$ adeded to second row, and all other rows are multiplied by zero

### Orthnogonal means(I): I = $A^TA$

### E.g.13 Let A be 2*2, orthnogonal, show $\exists \theta$ s.t.:

- A =  {$\cos{\theta}, \sin{\theta}\\ \sin{\theta}, -\cos{\theta}$}
- A $A^T$ = [1,0\\0,1] so it is

## L9

#### Proving stuff with inverse often involve multiplying the original matrix

#### if ad-bc != 0;

- $\begin{pmatrix} a & b\\c&d \end{pmatrix}$ is invertible with inverse $\frac{1}{ad-bc}\begin{pmatrix} d & -b\\-c&a \end{pmatrix}$

### Getting inverse:

- Suppose A is the 3*3 matrix to be inversed, B is a matrix $\begin{pmatrix}1&0&0\\0&1&0\\0&0&1\end{pmatrix}$
- (A| B). process it as if solveing a system of equations of A.
- If A becomes identity, B in the end is the inverse of A. Otherwise A is not invertible

## L10

##### Elementary matrix:

- identify matrix that went through three types of elementary rowo operation:
  - row switching
  - row multiplication
  - row addition
- ERO can be calculated by multiplying the matrix with a elementary matrix, with elementary matrix before the matrix itself

#### Determinant:

- Usage: For checking whether a matrix is invertible or not
- Definition: 
  - for invertible matrix, determinant is the matrix itself
  - else: e.g. $det\begin{pmatrix}a&b\\c&d\end{pmatrix} = ad-bc$
  - e.g. for 3*3 matrix: a(ei-fh)-d(bi-ch)+g(bf-ce). **Remember** consesecutive -'s
  - det of 4*4 $\to$ 4 dets of 3 *3 $\to$ $4\times3$ dets of 2 *2
- Techniques:
  1. det of upper $\triangle$ matrix: since others will be 0, just recursive top left corner's determinant, so u will get multiplying all numbers on diagonal in the end.

## L11

##### E.g.1  ERO type 3: to make $\begin{pmatrix}1&20\\3&40\end{pmatrix} \to \begin{pmatrix}1&2\\3&4\end{pmatrix}$

- $\det \begin{pmatrix}1&2\\3&4\end{pmatrix} = 1/10 \times \det \begin{pmatrix}1&20\\3&40\end{pmatrix}$
- $\det \begin{pmatrix}1&20\\3&40\end{pmatrix} = 10 \times \det \begin{pmatrix}1&2\\3&4\end{pmatrix}$

##### E.g.2 ERO type 2: $\begin{pmatrix}7&8&10\\4&5&6\\1&2&3\end{pmatrix} = -1 \times \begin{pmatrix}1&2&3\\4&5&6\\7&8&10\end{pmatrix}$

- =$-1\times \begin{pmatrix}7&8&10\\0&-3&-6\\1&2&3\end{pmatrix}$ Type 1
- =$-1\times -3 \times \begin{pmatrix}7&8&10\\0&1&2\\1&2&3\end{pmatrix}$ Type 3
- ... =  =$-1\times \begin{pmatrix}7&8&10\\0&-3&6\\0&0&1\end{pmatrix}$ Type 1
- =3

#### A is invertible $\Leftrightarrow$  $\det(a) != 0$

- Proof: if A is invertible,

  A $\to$ I (after EROS)
  $\det (E_k \times (E_{k-1)...E_1 \times A) = \det {I_n}$

  .. $\Leftrightarrow \det{A}!=0$

   If A is singular,
  A $\to B$ (after EROs), need to prove B = 0

  (hint: B must be upper $\triangle$ matrix)

#### Checking if linearly independent $\Leftrightarrow$ checking if determinant != 0 (only works for square matrix )

#### If A $\to (ERO) $B, E is elementary matrix, then

- B = EA
- det(EA) = det(B) = det(E)det(A)

## T6

#### How do EROs affect determinat:

| Type of det                              | effect on det     |
| ---------------------------------------- | ----------------- |
| Add $cr_n+r_m$                           | no effect         |
| swap $r_n$ with $r_m$                    | change sign       |
| scale a row i.e. multiply by c, c $\exists R$ | multiply det by c |

#### Cofactor expansion:

- Let $A_{ij}$ be the $(n-1)^2$ submatrix obtained by deleting the bla bla... tl:dr breaking break determinant to smaller

1. $\begin{vmatrix} 2&3\\0&4 \end{vmatrix}$
   = ad-bc = 8

2. ​

   1. $\begin{vmatrix}2&1\\1&2\end{vmatrix}$ i.e. A
      =3
   2. $\begin{vmatrix}-2&-1\\-1&-2\end{vmatrix}$ i.e. --A
      = 3
   3. det(2A)
      = $3\times2\times2=12$
   4. $\begin{vmatrix}1&-2\\-1&3\end{vmatrix}$
      = 1

3. Use cofactor expansion to compute the determinant $\begin{vmatrix}9&0&0&2\\7&3&2&8\\3&0&0&0\\5&-3&1&11\end{vmatrix}$

   ==*== Do cofactor expansion on row with more zeros. Here choose 3^rd^ row
   = 3$\begin{vmatrix}0&0&2\\3&2&8\\-3&1&11\end{vmatrix}$
   ... = 18

4. Given $\begin{vmatrix}a&b&c\\d&e&f\\g&h&i\end{vmatrix}=2$

   1. $\begin{vmatrix}g&h&i\\d&e&f\\a&b&c\end{vmatrix}$
      its a row change from A, so = -2
   2. $\begin{vmatrix}a+3g&b+3h&c+3i\\d&e&f\\g&h&i\end{vmatrix}=2$
      = 2 still

5. Determinant of matrix containing ___________

   1. a zero row
      = 0
   2. 2 same rows
      same as (1) = 0
   3. 2 proportional row
      same as (1) = 0

6. A = $\begin{vmatrix}a&b&c\\d&e&f\\g&h&i\end{vmatrix}$, B=$\begin{vmatrix}u&v&w\\d&e&f\\g&h&i\end{vmatrix}$, C=$\begin{vmatrix}a+u&b+v&c+w\\d&e&f\\g&h&i\end{vmatrix}$ . Find relationship between them
   A = $a\begin{vmatrix}e&f\\h&i\end{vmatrix}-b\begin{vmatrix}d&f\\g&i\end{vmatrix}+c\begin{vmatrix}d&e\\g&h\end{vmatrix}$ = a(ei-fh)-b(di-fg)+c(dh-eg)
   B = ... = u(ei-fh)-v(di-fg)+w(dh-eg)
   C = ... = (a+u)(ei-fh)-(b+v)(di-fg)+(c+w)(dh-eg) = A+B
   ==*== Their matrix do not follow the relationship tho

7. FInd $\begin{vmatrix}A\end{vmatrix} $ in terms of X. Find x s.t. A is not invertible

   1. $\begin{vmatrix}1&x&x^2&x^3\\x^3&1&x&x^2\\x^2&x^3&1&x\\x&x^2&x^3&1\end{vmatrix}$ can do row reduce first
      ... = $(1-x^4)^3$ = \begin{vmatrix}A\end{vmatrix} 

## L12

#### Cramers rule:

- matrix A has non-zereo determinent $\Leftrightarrow$ the matrix equation
  A $\begin{pmatrix}x_1\\x_2\\...\\x_n\end{pmatrix}=\begin{pmatrix}b_1\\b_2\\...\\b_n\end{pmatrix}$
- each solution of the matrix with be e.g. $x_2$ = $\begin{vmatrix}a_{11}&b_1 & a_{13}...\\a_{21}&b_2& a_{23}...\\...\end{vmatrix}$

### Vector Space

#### Subspace

- Define: Let V be a vector space. A subcollection of vectors in V , where H <= V is a vector subspce of V if H itself is a space of . That is: 
  - if $s_1, s_2$ are in H, then $s_1 + s_2 \in H$
  - if $s_1 \in H$, then $k s_1 \in H$


## L13

#### Prove something is/is not vector subspace

1. Get two matrixes $s_0, s_1$ from the proposed H
2. if det($s_0+s_1$)=0  , then it is vector subspace, else not

##### e.g. 1 (webwork 4__1?)

V = $R^3$

S = {$\begin{pmatrix}x\\x+4\\x+9\end{pmatrix}$|x$\in$R any no.}

e.g. $\begin{pmatrix}1\\1+4\\1+9\end{pmatrix}$ = $\begin{pmatrix}1\\5\\10\end{pmatrix} \in S$

however 0 $\times \begin{pmatrix}0\\4\\9\end{pmatrix}$ is not $\in S$

so S is not a vector subspace in R

#### e.g. 2 (webwork 4__3?)

V = Matrix of dimension n$\times$n

S = all diagonal matrices

Pick A = $\begin{pmatrix}a_1,...0\\0,a_2...0\\...\end{pmatrix} \in S$,B= $\begin{pmatrix}b_1,...0\\0,b_2...0\\...\end{pmatrix} \in S$ 

Since A+B is $\in S$ and c $\times$A $\in S$, it is a vector subspace

### Effective way to construct vector subspace of V

a vector collection S = {$v_1, v_2,...v_k...$}, are vectors in V

l.c. of S = $c_1v_1+c_2v_2....c_kv_k$

span S = all finite l.c. of S, is a vector subspace of V

#### e.g. 3(notes example) V = P (all polys)

take S = {1, $t_2, t^4..$}, then 10$t^4 + 25t^18 \in Span S$, more precisely Span S is collection of all even-powered polynomials

### Span(S) is always a vector subspace of V

Check:

- $s_1,s_2 \in Span(S) \implies s_1+s_2 \in Span(S)$
- $s_1 \in Span(S) \implies c \times s \in Span(S) for all c$

##### Finding Null space of matrix A means finding all solutions (i.e. collection of x $\in R^n$)of to Ax = 0. It will be vector subspace of $R^n$, as the solutions will be of form Span{$x_1,x_2...$}

##### Column space: $v \in$ col A $\Leftrightarrow$ [A|v] is consistent

##### Row space: row A = Span{$r_1^T, r_2^T...$}

e.g. A = $\begin{pmatrix}1&2&3\\4&5&6\end{pmatrix}$, row A = Span{$\begin{pmatrix}1\\2\\3\end{pmatrix}$,$\begin{pmatrix}4\\5\\6\end{pmatrix}$ }

#### Ways to simplify expression

- row A = col $A^T$
- row A = row B if B is A processed with ==ERO==
- col A = row B if B is A processed with ==ECO==

#### Linear transformation is not only T:$R^n \to R^m$, but in general transformation between vector spaces T:V $\to$ W satisfying;

- T(u+v) = Tu + Tw
- T(c u) = cTu

#### 'Kernel' of T (ker T) = all inputs $\underline{v}$ with output $\underline{o}$ 

## T7

#### e.g. 1 Relationship between det(A) and det($A^{-1}$) if A is invertible

det(A$A^{-1}$) = det(I)

$\implies$ det($A^{-1}$) = 1/det(A)

#### e.g. 2 detA=2, detB=3, detC=-1

i. det(ABA$C^T$) = 2$\times$3$\times$2$\times$(-1) =-12

since det($A^{-1}$) = 1/det(A) and det($A^T$)=det(A) and det($A^2$) = det(A)$^2$



#### e.g.3 Find detA, adjA, $A^{-1}$ where A = $\begin{pmatrix}1&1&0\\1&1&0\\0&2&1\end{pmatrix}$

$A^{-1} = 1/detA \times adj(A)$

when finding cofactor, ignore the magnitude and just get the signs of the each element to draw the cross from	

#### e.g. 4 let A,B be invertible matrices, show that:

i. adj(AB) = adj(B)adj(A)

since A$\times $ adj(A) = (detA) $I_n$

(AB) adj(AB) = adj(AB)(AB) = det(AB)$I_n$ (i.e. use A+AB)

(AB)adj(B)adj(A) = A(B adj(B))adj(A) = A(detB $I_n$)adjA  = detB $\times$ A $\times$ adjA = det(AB) = LHS

## L14

### Linearly independent for vectors:

set of vectors S= {$v_1,v_2,...$} is called linearly independent if $c_1v_1+c_2v_2+... = 0$ only has one, which is the trivial solution i.e. all $c_i$s has to be 0

e.g. if {u,v,w} is L.I., is S={u,u+2v,u+2v+3w} L.I.?

$\implies$ ($x_1+x_2+x_3$)u+($2x_2+2x_3$)v+3$x_3$w=0, solve $x_1,x_2,x_3$ with u,v,w are 0

#### Recall checking spans:

S spans V if matrix A = $\begin{pmatrix}v_1,v_2,...\end{pmatrix}$ has pivot on every row

##### if get $c_1\sin^2{x_1}+c_2\sin^2{x_2}=0$ then must be Linearly independent

#### A basis of subspace H of vector space V is a collection vectors B s.t. B is L.I. and Span B = H

##### Find why if a matrix of spans $R^m$ and is L.I, then it has RREF, and is a basis.

## L15

#### Proving a basis

1. Prove a general element of V is linear combination of B i.e. prove B spans V
2. solve all vectors in B adding to 0 with coefficients. Prove the only solution is trivial solutioin

##### Nul A: solutions of Ax=0, the basis are guaranteed to be L.I. if have free columns? (Notes e.g. 'Nul')

##### Basis of Nul(A): basic solutions of Ax=0

##### Basis of row(A):

1. A [ERO]$\to$ B
2. Basis of row(A) = non-zero rows of B

##### Basis of Col(A) = pivot columns of A

#### Warnings:

When talking about basis B, specify what vector space we are working with!

e.g. {$\begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\1\\0\end{pmatrix} $} is not a basis, is not a basis of V=$R^3$, but is a basis of H=span{$\begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\1\\0\end{pmatrix} $}

#### Coordinate vector

##### Aim of coordinate vectors is to translate questions in vector space V into questions in $R^m$

Theorem: if B = {$b_1,b_2,...$} is basis of vector subspace H, then every x $\in$ H is x=$c_1b_1+c_2b_2+...$

##### How to make coordinate vector (orders of the vectors in B matters)

Theorem: if B = {$b_1,b_2,...$} is basis of vector subspace H, and every x $\in$ H is x=$c_1b_1+c_2b_2+...$,

then the B-coordinate vector of x is defined as [x]$_B$ = $\begin{pmatrix}c_1\\c_2\\..\end{pmatrix} \in R^p$

this way we can use vectors in $R^p$ to study vector subspace H

## T11

#### 1. Let V be the collection of all ordered pairs...

Check from definition: let u=($x_1,y_1$), y=($x_2,y_2$), z=($x_3,y_3$)

u $\oplus$ v $\in$ V? ($x_1+x_2, y_1+y_2$) $\in$ V because the x's and y's are $\in$V

u $\oplus$ v = v $\oplus$ w? yes 

(u + v) + w = u + (v+ w)

#### 2. If H = {[$\begin{matrix}x\\y\end{matrix}$]:x>= 0}, is H subspace of $R^2$?

H is subspace of V if:

1. $0_v \in H$
2. a,b $\in$ H $\implies$ a+b $\in$ H. a,b are vectors
3. if c $\in$ R, $\implies$ cb $\in$ H

here we choose (3) to fulfill. say we pick c = -1, x $\in$ H where x = $[\begin{matrix}x_1\\y_1\end{matrix}]$, assume $x_1$ > 0,

cx =  $[\begin{matrix}-x_1\\-y_1\end{matrix}]$ $\implies -x_1 < 0$, so cx is not $in$ H $\implies$ H is not subspace of V

#### 4. ...

If $A^T=-A \Leftrightarrow$ skew symmetric

Let A, B $\in$ H

1. $O_{M_{n\times n}} \in K$? yes since it is vector and skew symetric
2. A+B $\in$ H? yes since the tranpose of A+B is equal to -A-B, and is $\in$H
3. check scaling: cA $\in$H ? yes since $(cA)^T$ = -cA $\in$H 

therefore H is subspace of K

#### 5. Let H be collection of non-invertable $n\times n$ matrix, is H subsace of M?

A is non-invertible $\Leftrightarrow$ det(A)=0

check if A,B $\in$ H? $\implies$ A+B $\in$ H?

consider A=$[\begin{matrix}2&6\\1&3\end{matrix}] \in H$, B = $[\begin{matrix}2&6\\1&3\end{matrix}] \in H$  because det(A)=det(B)=0

det(A+B) = -2 != 0, so H is not subspace of M

#### 6. ...

what about K={q(t)|q(b)=1}? No.

1. $0_p \in H$? yes since p(b)=0 $\in$ H
2. will be proved in 3
3. let p($t_1$), p($t_2$) $\in$ H. ap(b)+bp(b)=a$\cdot$0 + b $\cdot$ 0. These two objects sill have a zero at b $\implies$ H is subspace of P

#### 8. ...

tan x = a sin x= b cos x

tan x = $\frac{sin x }{cos x}$ $\exists$ a,b to satisfy a,b$\in$R $\implies$ tan x is not L.C. of S

#### 9. ...

from Euler's ID: $e^x$= cos x+ i sin x

but i is not $\in$R $\implies$ $e^x is not a L.C. either$

#### 10. Expres Nul A as a span of a suitablle set of vectors. A = $[\begin{matrix}1&0&2\\0&1&-1\end{matrix}]$

Nul A $\implies$ solve [A|0] $\implies$ find some basis

1. Put in RREF
2. [A|0] $\implies$ $x_1=-2x_3$ and $x_2=x_3$
3. write basis. $x_3$ is free, choose some value for it. we choose $x_3$ = 1 $\implies$ $x_1$ = -2 and $x_2$ = 1 $\implies basis is t[\begin{matrix}-2\\1\\1\end{matrix}]$, where t $\in$ R
4. span is {$\begin{pmatrix}-2\\1\\1\end{pmatrix}$}

#### 11. ...

Definition: Let A=[$a_1,a_2...$] be m $\times$n matrix. the a's are cols.

column space is col A = span {$a_1,a_2,...$}. col A is subspace of $R^m$

Theorem: V $\in$ col A $\Leftrightarrow$ [A|v] is consistent

Answer is No

#### 12. 

Theorem: [$A^T$|v] is consistent $\Leftrightarrow$ v $\in$ row A

Answer is yes



## L16

###### Check if B is basis of V $\Leftrightarrow$ check if B as collection of vectors is basis of $R^3$ $\Leftrightarrow$ check if B as a matrix has picots on every row and column

#### Dimension of Vector space (dim V):

All Basis of a vector space must have same number of elements. It is also the dimension of V

##### Whether a set S is basis of V, where dim V = n:

- L.I. S with n elements
- spanning set S with n elements

###### Dimension jargons of Nul A, Col A, Row A:

- dimension of Nul A: nullity of A
- dimension of Col A: column rank of A
- dimension of Row A: row rank of A
- (Nullity of A) + (Rank of A) = n

##### dim ker(T) + dim im(T) = n

### Eigenvectors(week 10)

Definition: Matrix A has ==eigenvector== x $\in R^n$ with eigenvalue b if Ax=$\lambda$x for non-zero vector x and scalar $\lambda$

###### E.g. suppose A is invertible, and has eigenvector x , then 16A has a eigenvecor x with eigenvalue=?

16A(x)=16(Ax)

=16($\lambda$x)

=(16$\lambda$)x

Eigenvalue = 16$\lambda$

##### Characteristic equation: det(A-$\lambda$I)=0. The roots give all eigenvalues of A



